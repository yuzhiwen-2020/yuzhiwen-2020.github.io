---
title: 日志集成工具EFK
date: 2023-10-23 10:30:00 +0800
categories: [k8s,日志集成]
tags: [fluentd, EFK]
author: elon
mermaid: true
---
# 日志集成工具EFK

首先摆出整体架构

!["选型对比的图片"](/assets/imgs/k8s/efk/efk-整体架构.png)
# 选型对比

日志集成目前主流是elk和efk
其中e代表es
l代表logstash、k代表kibana、f代表filebeat
由于logstash对资源要求高，weiyun采用的是efk方案；

Fluentd是一个流行的开源数据收集器，我们将在 Kubernetes 集群节点上安装 Fluentd（代替filebeat），通过获取容器日志文件、过滤和转换日志数据，然后将数据传递到 Elasticsearch 集群，在该集群中对其进行索引和存储  

分布式日志采集系统，可以从不同的服务，数据源采集日志，对日志进行过滤加工，分发给多种存储和处理系统。支持各种插件，数据缓存机制，且本身所需的资源很少，内置可靠性，结合其他服务，可以形成高效直观的日志收集平台

# 验证
做个搜集日志的集成demo  

## 前置条件
安装docker  
安装k3s集群1master+2slave  
` kubectl get nodes `
!["nodes信息"](/assets/imgs/k8s/efk/efk-nodes信息.png)
## ns创建
本次验证新建一个ns喂logging-208，后续的kibana和fluentd都在本ns创建  
logging-208-ns.yaml  
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: logging-208
```
```shell
kubectl apply -f logging-208-ns.yaml  
kubectl get ns
```
!["ns信息"](/assets/imgs/k8s/efk/efk-ns.png)
## es集群创建
es 集群需要奇数个，新建3个节点  
首先创建es的服务（无头）：  
*为什么用无头？es节点时需要域名解析为不同的ip，无需用集群的负载均衡策略*  
elasticsearch-svc.yaml  
```shell
kubectl apply -f elasticsearch-svc.yaml
```
```yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch
  namespace: logging-208
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  clusterIP: None
  ports:
    - port: 9200
      name: rest
    - port: 9300
      name: inter-node
```

现在我们已经为 Pod 设置了无头服务和一个稳定的域名elasticsearch.logging-208.svc.cluster.local，接下来我们通过 StatefulSet 来创建具体的 Elasticsearch 的 Pod 应用。  
Kubernetes StatefulSet 允许我们为 Pod 分配一个稳定的标识和持久化存储，Elasticsearch 需要稳定的存储来保证 Pod 在重新调度或者重启后的数据依然不变，所以需要使用 StatefulSet 来管理 Pod  

!["es-svc信息"](/assets/imgs/k8s/efk/efk-es-svc.png)  
elasticsearch-statefulset.yaml

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es
  namespace: logging-208
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels: 
        app: elasticsearch
    spec:
      initContainers:
      - name: increase-vm-max-map
        image: busybox
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      - name: increase-fd-ulimit
        image: busybox
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2
        ports:
        - name: rest
          containerPort: 9200
        - name: inter-node
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 1000m
        volumeMounts: 
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
        - name: cluster.name
          value: k8s-logs
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: cluster.initial_master_nodes
          value: "es-0,es-1,es-2"
        - name: discovery.zen.minimum_master_nodes
          value: "2"
        - name: discovery.seed_hosts
          value: "elasticsearch"
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"
        - name: network.host
          value: "0.0.0.0"
      volumes: 
      - name: data
        emptyDir: {}
```
说明：因本地测试，所以volume挂载了emptydir，如在正式环境，需要替换为证实的分布式存储；后期在test环境验证时需要改为分布式存储  
!["es-pod信息"](/assets/imgs/k8s/efk/efk-es-sf.png)  
`
kubectl describe svc elasticsearch -n logging-208
`  

## kibana创建

## fluentd集群创建
### rbac授权
### 配置信息说明
## 日志查看
# 思考
### 插件的输出方式
### 高可用
#### 语义
#### 数据丢失的场景
#### 监控
### 性能调优
# 参考
